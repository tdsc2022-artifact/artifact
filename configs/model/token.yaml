# @package _global_
name: token

dataset:
  name: example
  token:
    max_parts: 200
    is_wrapped: false
    is_splitted: false
    vocabulary_size: 190000

encoder:
  embedding_size: 50
  rnn_size: 256
  use_bi_rnn: true
  embedding_dropout: 0.25
  rnn_num_layers: 1
  rnn_dropout: 0.5

classifier:
  n_hidden_layers: 2
  hidden_size: 256
  classifier_input_size: 256
  activation: tanh

hyper_parameters:
  n_epochs: 20
  reload_dataloader: true
  patience: 5
  batch_size: 64
  test_batch_size: 64
  clip_norm: 0
  random_context: true
  shuffle_data: true

  optimizer: "Adamax"
  learning_rate: 0.01